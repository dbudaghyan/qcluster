# Evaluation results description

Once the pipeline has been executed, the evaluation results will be available in the
`EVALUATION_RESULTS_DIR`.

## Evaluation Results Directory

The evaluation results directory contains one folder for each evaluation run.
Each folder is named with the timestamp of the evaluation run and the git hash of the
commit that was evaluated.

## Results Structure

Each evaluation run folder contains the following files:

- `.env`: Environment variables used during the evaluation.
- `cluster_to_class_scores.csv`: A CSV file containing the statistics related to cluster
  to class similarity.
- `clusters.json`: A JSON file containing the clusters found during the evaluation,
  including their titles, descriptions, and the number of documents in each cluster.
- `final_report.md`: A Markdown file containing an autogenerated report of the evaluation.
- `git_diff.txt`: A text file containing the difference between the latest state of the
  repository and the latest commit.
- `main_script_*.py/ipynb`: The entrypoint script or notebook executed during the
  evaluation.
- `pycm.zip`: A ZIP file containing the confusion matrix information. This file can be
  deserialized
  using the `pycm` library and be used for reproducing the confusion matrix.
- `results.html`: A user-friendly HTML report of the confusion matrix.
- `stats.csv`: A CSV file containing comprehensive statistics of the evaluation, including
  precision,
  recall, F1 score, and other metrics.
- `stats.pycm`: A text file containing the summary statistics of the confusion matrix,
  including precision, recall, F1 score, and other metrics.
- `stats_matrix.csv`: Raw confusion matrix without any statistics and labels.
