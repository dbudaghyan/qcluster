{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering and Analysis Pipeline\n",
    "\n",
    "This notebook walks through a pipeline for clustering text data, specifically customer support training data. The process involves several key stages:\n",
    "\n",
    "1.  **Data Loading**: Load a dataset of customer support interactions from a CSV file.\n",
    "2.  **Feature Extraction**: Convert text into numerical representations (embeddings) and then reduce the dimensionality of these embeddings to make them more suitable for clustering.\n",
    "3.  **Instruction Generation**: Create a set of instructions from the sample data.\n",
    "4.  **Clustering**: Group the generated instructions into clusters based on their semantic similarity.\n",
    "5.  **Cluster Description & Matching**: Generate a descriptive title for each cluster and match it back to the original sample categories.\n",
    "6.  **Evaluation**: Assess the quality of the clustering using a confusion matrix and other similarity scores.\n",
    "\n",
    "Each step is contained in its own cell, with explanations of the code and guidance on how to modify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "First, we import all the necessary libraries and modules. This includes utilities for data handling, logging, machine learning models, and our custom pipeline functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Preloading env vars, seeds and models\n",
    "import os\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import time\n",
    "from os import PathLike\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "from qcluster import tqdm\n",
    "from tqdm import tqdm\n",
    "from qcluster import ROOT_DIR\n",
    "from qcluster.preload import MODEL\n",
    "\n",
    "# Clustering algorithms\n",
    "from qcluster.algorithms.clustering import (\n",
    "    kmeans_clustering,\n",
    "    # dbscan_clustering,\n",
    "    # hdbscan_clustering,\n",
    "    # agglomerative_clustering,\n",
    "    # bert_topic_extraction,\n",
    "    # spectral_clustering\n",
    ")\n",
    "\n",
    "# Feature extractors and dimensionality reduction\n",
    "from qcluster.algorithms.feature_extractors import (\n",
    "    create_embeddings,\n",
    "    # pca_reduction,\n",
    "    umap_reduction,\n",
    "    # pacmap_reduction\n",
    ")\n",
    "\n",
    "# Data models and custom types\n",
    "from qcluster.custom_types import CategoryType, IdToCategoryResultType\n",
    "from qcluster.datamodels.instruction import InstructionCollection\n",
    "from qcluster.datamodels.sample import SampleCollection\n",
    "\n",
    "# Other pipeline components\n",
    "from qcluster.llm.describer import get_description\n",
    "from qcluster.evaluation import evaluate_results, cluster_to_class_similarity_measures\n",
    "from qcluster.algorithms.similarity import get_top_n_similar_embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters\n",
    "\n",
    "This section defines the core components of our pipeline and their parameters. By modifying the variables in this cell, you can easily experiment with different algorithms and settings.\n",
    "\n",
    "### How to Modify:\n",
    "-   **`clustering_function`**: To change the clustering algorithm, comment out `kmeans_clustering` and uncomment another option like `hdbscan_clustering`. Adjust the parameters accordingly. For instance, K-Means requires `n_clusters`, while HDBSCAN uses `min_cluster_size`.\n",
    "-   **`feature_extractor`**: To change the dimensionality reduction technique, replace `umap_reduction` with another imported function like `pca_reduction` or `pacmap_reduction`. You can also adjust the `n_components` parameter, which determines the final number of dimensions for the embeddings.\n",
    "-   **`similarity_function`**: You can toggle `use_mmr` (Maximal Marginal Relevance) to control the diversity of similarity search results. When `use_mmr=True`, you can also tune the `mmr_lambda` parameter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Path to the dataset\n",
    "CSV_PATH: PathLike = (\n",
    "        ROOT_DIR.parent\n",
    "        / \"data\"\n",
    "        / \"Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n",
    ")\n",
    "\n",
    "# Dynamically determine the number of categories from the data\n",
    "N_CATEGORIES = len(SampleCollection.all_category_classes())\n",
    "logger.info(f\"Found {N_CATEGORIES} unique categories.\")\n",
    "\n",
    "# --- Clustering Algorithm Configuration ---\n",
    "clustering_function = functools.partial(\n",
    "    kmeans_clustering,\n",
    "    n_clusters=N_CATEGORIES\n",
    "    # --- Alternative: HDBSCAN ---\n",
    "    # hdbscan_clustering,\n",
    "    # min_cluster_size=15, # Minimum size for a group to be considered a cluster\n",
    ")\n",
    "\n",
    "# --- Cluster Describer Configuration ---\n",
    "describer = functools.partial(\n",
    "    get_description,\n",
    "    template_name='description_prompt_simple',\n",
    "    # --- Alternative: More detailed template ---\n",
    "    # template_name='description_prompt_from_instructions'\n",
    ")\n",
    "\n",
    "# --- Similarity Function Configuration ---\n",
    "similarity_function = functools.partial(\n",
    "    get_top_n_similar_embeddings,\n",
    "    use_mmr=False,\n",
    "    # --- MMR Parameters (for diversifying results) ---\n",
    "    # mmr_lambda=0.3, # Set between 0 (focus on similarity) and 1 (focus on diversity)\n",
    "    # mmr_top_n=20\n",
    ")\n",
    "\n",
    "# --- Feature Extraction Pipeline ---\n",
    "def feature_extractor(texts: list[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given texts and reduces their dimensionality.\n",
    "    \"\"\"\n",
    "    # Step 1: Create high-dimensional embeddings from text\n",
    "    embeddings = create_embeddings(texts, model=MODEL)\n",
    "    \n",
    "    # Step 2: Reduce dimensionality\n",
    "    # Recommended to use a value between 5 and 50 for n_components\n",
    "    reduced_embeddings = umap_reduction(embeddings, n_components=28)\n",
    "    return reduced_embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Functions\n",
    "\n",
    "Here we define the functions that encapsulate each major step of the pipeline, from loading data to evaluating the final results. This modular approach makes the process easy to follow and debug."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_samples(path: PathLike) -> SampleCollection:\n",
    "    \"\"\"Loads samples from a CSV file.\"\"\"\n",
    "    logger.info(f\"Loading samples from {path}...\")\n",
    "    samples = SampleCollection.from_csv(path)\n",
    "    logger.info(f\"Loaded {len(samples)} samples.\")\n",
    "    return samples\n",
    "\n",
    "def process_samples(samples: SampleCollection) -> dict[CategoryType, SampleCollection]:\n",
    "    \"\"\"Groups samples by category and updates their embeddings and descriptions.\"\"\"\n",
    "    logger.info(\"Grouping samples by category and updating embeddings...\")\n",
    "    samples_by_category = samples.group_by_category()\n",
    "    logger.info(f\"Grouped samples into {len(samples_by_category)} categories.\")\n",
    "\n",
    "    logger.info(\"Describing samples in each category...\")\n",
    "    for category, sample_collection in tqdm(samples_by_category.items()):\n",
    "        sample_collection.update_embeddings(feature_extractor)\n",
    "        sample_collection.describe(describer)\n",
    "    logger.info(\"Embeddings updated and samples described.\")\n",
    "    return samples_by_category\n",
    "\n",
    "def create_instructions(samples: SampleCollection) -> InstructionCollection:\n",
    "    \"\"\"Creates and processes an InstructionCollection from a SampleCollection.\"\"\"\n",
    "    logger.info(\"Creating instruction collection from samples...\")\n",
    "    instructions = InstructionCollection.from_samples(samples)\n",
    "    logger.info(f\"Created an instruction collection with {len(instructions)} instructions.\")\n",
    "\n",
    "    logger.info(\"Updating instruction embeddings and clustering...\")\n",
    "    (\n",
    "        instructions\n",
    "        .update_embeddings(feature_extractor)\n",
    "        .update_clusters(clustering_function=clustering_function, use_raw_instructions=False)\n",
    "    )\n",
    "    logger.info(\"Instruction embeddings updated and clusters created.\")\n",
    "    return instructions\n",
    "\n",
    "def create_and_match_clusters(\n",
    "        instructions: InstructionCollection,\n",
    "        samples_by_category: dict[CategoryType, SampleCollection],\n",
    "        all_samples: SampleCollection\n",
    ") -> IdToCategoryResultType:\n",
    "    \"\"\"Describes instructions and matches them to sample categories.\"\"\"\n",
    "    logger.info(\"Grouping instructions by cluster...\")\n",
    "    instructions_by_cluster = instructions.group_by_cluster()\n",
    "    logger.info(f\"Grouped instructions into {len(instructions_by_cluster)} clusters.\")\n",
    "\n",
    "    logger.info(\"Describing instructions in each cluster...\")\n",
    "    for cluster, instruction_collection in tqdm(instructions_by_cluster.items()):\n",
    "        instruction_collection.describe(describer)\n",
    "    logger.info(\"Instructions described.\")\n",
    "\n",
    "    logger.info(\"Finding top similar sample categories for each instruction cluster...\")\n",
    "    id_to_category_pairs: IdToCategoryResultType = {}\n",
    "    for cluster, instruction_collection in tqdm(instructions_by_cluster.items()):\n",
    "        predicted_category = instruction_collection.get_cluster_category(\n",
    "            sample_collections=list(samples_by_category.values()),\n",
    "            similarity_function=similarity_function,\n",
    "        )\n",
    "        logger.info(f\"Cluster N {instruction_collection.cluster} title: `{instruction_collection.title}` top similar sample category: {predicted_category}\")\n",
    "        logger.info(f\"Mapping cluster {cluster} to category {predicted_category}\")\n",
    "        for sample in instruction_collection:\n",
    "            id_to_category_pairs[sample.id] = (\n",
    "                all_samples.get_sample_by_id(sample.id).category,\n",
    "                predicted_category\n",
    "            )\n",
    "    logger.info(\"Matching completed.\")\n",
    "    logger.info(f\"Total pairs: {len(id_to_category_pairs)}\")\n",
    "    return id_to_category_pairs\n",
    "\n",
    "def store_results(cm: ConfusionMatrix, cluster_to_class_scores, storage_path: PathLike):\n",
    "    \"\"\"Saves the confusion matrix and scores to CSV files.\"\"\"\n",
    "    storage_path = Path(storage_path)\n",
    "    os.makedirs(os.path.dirname(storage_path), exist_ok=True)\n",
    "    logger.info(f\"Storing evaluation results to {storage_path}...\")\n",
    "    cm.save_csv(storage_path / \"confusion_matrix.csv\")\n",
    "    # Note: Storing cluster_to_class_scores would require converting the dict to a file format like json or csv.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute the Pipeline\n",
    "\n",
    "This is the main execution block. It calls the pipeline functions in sequence and prints the final evaluation metrics. \n",
    "\n",
    "### Note on Subsetting Data:\n",
    "To run the pipeline on a smaller portion of the data for faster testing, you can uncomment the line `samples: SampleCollection = samples[:1000]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the clustering pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Step 1: Load Data ---\n",
    "    samples = load_samples(CSV_PATH)\n",
    "    \n",
    "    # --- Optional: Uncomment to use a smaller subset of data for quick tests ---\n",
    "    # samples: SampleCollection = samples[:1000]\n",
    "    logger.info(f\"Using {len(samples)} samples for processing.\")\n",
    "\n",
    "    # --- Step 2: Process Samples ---\n",
    "    samples_by_category = process_samples(samples)\n",
    "    \n",
    "    # --- Step 3: Create Instructions and Cluster Them ---\n",
    "    instructions = create_instructions(samples)\n",
    "    \n",
    "    # --- Step 4: Match Clusters to Categories ---\n",
    "    id_to_category_pairs = create_and_match_clusters(\n",
    "        instructions, samples_by_category, samples\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Evaluate Results ---\n",
    "    logger.info(\"Evaluating results...\")\n",
    "    cm = evaluate_results(id_to_category_pairs)\n",
    "    \n",
    "    # Prepare data for similarity score calculation\n",
    "    predicted_cluster_list = []\n",
    "    actual_category_list = []\n",
    "    for id_, (actual_category, predicted_category) in id_to_category_pairs.items():\n",
    "        predicted_cluster_list.append(predicted_category)\n",
    "        actual_category_list.append(actual_category)\n",
    "        \n",
    "    # Calculate and print clustering quality scores\n",
    "    cluster_to_class_scores = cluster_to_class_similarity_measures(\n",
    "        predicted_cluster_list, actual_category_list\n",
    "    )\n",
    "    # Print the confusion matrix and detailed statistics\n",
    "    logger.info(\"Evaluation results:\")\n",
    "    for measure, score in cluster_to_class_scores.items():\n",
    "        print(f\"{measure.capitalize()}: {score:.4f}\")\n",
    "    cm.print_matrix(sparse=True)\n",
    "    cm.stat(summary=True)\n",
    "        store_results(\n",
    "        cm,\n",
    "        cluster_to_class_scores,\n",
    "        storage_path=Path(os.environ[\"EVALUATION_RESULTS_DIR\"])\n",
    "                     / f\"results_{unique_folder_name}\"\n",
    "    )\n",
    "    logger.info(f\"Execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
